{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Angel-Castro-RC/NLP/blob/main/F4_2_WordSenseDisambiguation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C192SOmJS6lw"
      },
      "source": [
        "# CS 195: Natural Language Processing\n",
        "## WordSense Disambiguation\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ericmanley/f23-CS195NLP/blob/main/F4_2_WordSenseDisambiguation.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uf-4jMfWlMJ3"
      },
      "source": [
        "## References\n",
        "\n",
        "Word Senses and WordNet, Chapter 23 of *Speech and Language Processing* by Daniel Jurafsky & James H. Martin: https://web.stanford.edu/~jurafsky/slp3/23.pdf\n",
        "\n",
        "WordNet documentation: https://www.nltk.org/api/nltk.corpus.reader.wordnet.html\n",
        "\n",
        "SemCor Corpus Module documentation: https://www.nltk.org/api/nltk.corpus.reader.semcor.html\n",
        "\n",
        "NLTK Stopwords: https://pythonspot.com/nltk-stop-words/\n",
        "\n",
        "Lemmatization with NLTK: https://www.geeksforgeeks.org/python-lemmatization-with-nltk/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Uc4j0Yy-lMJ3",
        "outputId": "57b5dce0-b527-4099-bc85-0e1354db2bb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NoVjKs6AlMJ5"
      },
      "outputs": [],
      "source": [
        "#you shouldn't need to do this in Colab, but I had to do it on my own machine\n",
        "#in order to connect to the nltk service\n",
        "import nltk\n",
        "import ssl\n",
        "\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHc2Ka89lMJ5"
      },
      "source": [
        "## Word Sense Disambiguation\n",
        "\n",
        "As we explored last time, one word can have many *senses*.\n",
        "\n",
        "The **WordNet** database can be used to look up different word senses of a particular word.\n",
        "\n",
        "The task of figuring out which sense is being usede in a given context is called **word sense disambiguation**\n",
        "\n",
        "Important for\n",
        "* extracting proper meaning from text\n",
        "* translation - e.g., different senses of one word in English might have different translations\n",
        "* question answering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qT0UACflMJ5"
      },
      "source": [
        "## Typical approach for WSD\n",
        "\n",
        "Look at the *context* of a word - what other words are around it\n",
        "\n",
        "For example, consider the word **bank** in\n",
        "\n",
        "\"I need to go to the bank and deposit my paycheck.\"\n",
        "\n",
        "We can determine from *deposit*, *paycheck*, and maybe even *go to* that we're talking about a financial institution and not a river bank.\n",
        "\n",
        "Which definition does the context share the most words with?\n",
        "\n",
        "*Definition 1:* 'sloping land (especially the slope beside a body of water)'\n",
        "\n",
        "*Definition 2:* 'a financial institution that accepts deposits and channels the money into lending activities'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "9AMIgZM5lMJ6",
        "outputId": "36f41c4b-f00d-44d1-c6bc-e69412f50cdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "def compute_overlap(set1, set2):\n",
        "    count_overlap = 0\n",
        "    for item in set1:\n",
        "        if item in set2:\n",
        "            count_overlap += 1\n",
        "    return count_overlap\n",
        "\n",
        "\n",
        "sentence = [\"i\", \"need\", \"to\", \"go\", \"to\", \"the\", \"bank\", \"and\", \"deposit\", \"my\", \"paycheck\"]\n",
        "definition1 = [\"sloping\", \"land\", \"especially\", \"the\", \"slope\", \"beside\", \"a\", \"body\", \"of\", \"water\"]\n",
        "definition2 = [\"a\", \"financial\", \"institution\", \"that\", \"accepts\", \"deposits\", \"and\", \"channels\", \"the\", \"money\", \"into\", \"lending\", \"activities\"]\n",
        "\n",
        "print( compute_overlap(sentence,definition1) )\n",
        "print( compute_overlap(sentence,definition2) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkgFOWfAlMJ7"
      },
      "source": [
        "### Discuss: What problems do you see with this approach?\n",
        "\n",
        "some words are similar but just by a letter is not the same and also it just match in simple words like \"the' \"and\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7iI1fOtlMJ7"
      },
      "source": [
        "## The Simplified Lesk Algorithm\n",
        "\n",
        "The **Simplified Lesk Algorithm** loops over all possible word senses to find the one whose definition/examples share the most words in common with the sentence context.\n",
        "\n",
        "Given a `word` and `sentence`\n",
        "1. Make a *set* of all the words in the sentence (my need to tokenize)\n",
        "2. Look up all the `synsets` for `word` in **WordNet**\n",
        "3. Loop through the list of `synsets`\n",
        "    * create a signature - the set of all the words that appear the definition and list of examples for this `word` from **WordNet** (may need to tokenize)\n",
        "    * compute the overlap between the signature and the word context\n",
        "    * if this is better than the previous best overlap, save the new sense"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsOxpKGflMJ7"
      },
      "source": [
        "### Discuss: How should we tokenize our text data for this problem?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "TS4if8TGlMJ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47702c5e-c5d5-45fa-f488-bc900968b544"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'p', 'b', 'e', 'g', 'h', 'c', 'k', 'n'}\n",
            "[Synset('bank.n.01'), Synset('depository_financial_institution.n.01'), Synset('bank.n.03'), Synset('bank.n.04'), Synset('bank.n.05'), Synset('bank.n.06'), Synset('bank.n.07'), Synset('savings_bank.n.02'), Synset('bank.n.09'), Synset('bank.n.10'), Synset('bank.v.01'), Synset('bank.v.02'), Synset('bank.v.03'), Synset('bank.v.04'), Synset('bank.v.05'), Synset('deposit.v.02'), Synset('bank.v.07'), Synset('trust.v.01')]\n",
            "{'river', ')', 'bank', 'current', 'body', '(', 'slope', 'pulled', 'sat', 'canoe', 'especially', 'sloping', 'land', 'watched', 'beside', 'water'}\n",
            "0\n",
            "{'lending', 'institution', 'hold', 'mortgage', 'cashed', 'money', 'check', 'home', 'financial', 'deposit', 'activity', 'accepts', 'channel', 'bank'}\n",
            "0\n",
            "{'earth', 'huge', 'ridge', 'long', 'pile', 'bank'}\n",
            "0\n",
            "{'tier', 'row', 'switch', 'object', 'arrangement', 'operated', 'similar', 'bank'}\n",
            "0\n",
            "{'supply', 'use', ')', '(', 'emergency', 'stock', 'future', 'reserve', 'especially', 'held'}\n",
            "0\n",
            "{'break', 'gambling', 'game', 'monte', 'house', 'carlo', 'fund', 'dealer', 'held', 'tried', 'bank'}\n",
            "0\n",
            "{'effect', 'higher', 'turn', 'centrifugal', ';', 'reduce', 'outside', 'force', 'track', 'slope', 'inside', 'order', 'road'}\n",
            "0\n",
            "{'coin', 'container', ')', '(', 'top', 'empty', 'slot', 'keeping', 'money', 'home', 'usually', 'bank'}\n",
            "0\n",
            "{'witherspoon', 'corner', 'business', 'banking', 'transacted', 'nassau', 'building', 'bank'}\n",
            "0\n",
            "{'aircraft', 'axis', 'went', ')', 'turning', 'tip', '(', 'steep', 'longitudinal', 'plane', 'laterally', 'flight', 'especially', 'maneuver', ';', 'bank'}\n",
            "0\n",
            "{'aircraft', 'tip', 'pilot', 'laterally', 'bank'}\n",
            "0\n",
            "{'enclose', 'road', 'bank'}\n",
            "0\n",
            "{'business', 'town', 'account', 'keep', '?', 'bank'}\n",
            "0\n",
            "{'gambling', 'banker', 'act', 'game'}\n",
            "0\n",
            "{'banking', 'business'}\n",
            "0\n",
            "{'every', 'account', 'paycheck', 'month', 'deposit', 'put', 'bank'}\n",
            "0\n",
            "{'burning', 'control', 'cover', 'rate', 'fire', 'ash', 'bank'}\n",
            "0\n",
            "{'trust', \"'s\", 'swear', 'faith', 'god', 'confidence', 'education', 'good', 'friend', 'grandmother', 'rely', 'recipe', 'bank'}\n",
            "0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Synset('trust.v.01')"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "# nltk.download('wordnet') #only need to do this once\n",
        "# nltk.download(\"punkt\")\n",
        "# nltk.download('stopwords') #only need to do this once\n",
        "stops = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "file_ids = semcor.fileids()\n",
        "sentences = semcor.sents(file_ids[0])\n",
        "tags = semcor.tagged_sents(file_ids[0], tag=\"sem\")\n",
        "def simplified_lesk(word,sentence):\n",
        "    best_sense = None\n",
        "    best_sense_overlap = 0\n",
        "\n",
        "    sentence_punc = \" \".join(sentence).lower().replace(\".\", \"\")\n",
        "    tokenized_word = word_tokenize(sentence_punc)\n",
        "    context = set(tokenized_word)\n",
        "    context = {word for word in context if word not in stops}\n",
        "    context = {lemmatizer.lemmatize(word) for word in context}\n",
        "    print(context)\n",
        "\n",
        "    # print(context)\n",
        "    word_synsets = wn.synsets(word)\n",
        "    print(word_synsets)\n",
        "\n",
        "    for sense in word_synsets:\n",
        "      signature = set(word_tokenize(sense.definition()))\n",
        "      for example in sense.examples():\n",
        "        signature.update(word_tokenize(example.lower()))\n",
        "      signature = {word for word in signature if word not in stops }\n",
        "      signature = {lemmatizer.lemmatize(word) for word in signature}\n",
        "      print(signature)\n",
        "      current_overlap = compute_overlap(context,signature)\n",
        "      print(current_overlap)\n",
        "\n",
        "      if current_overlap >= best_sense_overlap:\n",
        "        best_sense_overlap = current_overlap\n",
        "        best_sense = sense\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #fill this in\n",
        "\n",
        "    return best_sense\n",
        "\n",
        "simplified_lesk(\"bank\", \"I need to go to the banks and deposit my paycheck\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOsEQiRTlMJ8"
      },
      "source": [
        "### Group Exercise: Finish implementing this algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIc4O5ozlMJ8"
      },
      "source": [
        "## Improving the algorithm\n",
        "\n",
        "Two things we could do to try to improve the Lesk algorithm\n",
        "\n",
        "1. Remove tokens that don't carry meaning like punctuation and *stopwords* (words like \"the\", \"is\", \"to\", etc.)\n",
        "\n",
        "2. Lemmatize the words - convert them into their base form\n",
        "\n",
        "Try to catch the word \"deposit(s)\" in\n",
        "* \"a financial institution that accepts **deposits** and channels the money into lending activities'\n",
        "* \"I need to go to the bank and **deposit** my paycheck.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPrXRFD6lMJ8"
      },
      "source": [
        "## Stopwords Corpus\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "vrYeJFh0lMJ8",
        "outputId": "a2e62a90-ac76-474d-9d63-37f66aac5b58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'mightn', 'most', 'not', \"she's\", 'did', 'at', \"hasn't\", 'if', 'needn', 'won', 'both', 'these', 'it', 'weren', 'was', 'ourselves', 'its', 'they', 'then', \"you'd\", 'this', 'ain', 'wasn', 'am', 'couldn', 'only', \"won't\", 'our', 'are', \"mustn't\", 'his', \"doesn't\", 'has', 'very', 'between', 'up', 'all', \"needn't\", 'nor', \"couldn't\", 'while', 'aren', 'over', 'own', 'don', 'those', 'that', 've', \"mightn't\", 'here', 'again', 'myself', 'o', 'mustn', 'after', 'd', 'me', 'is', 'each', 'haven', 'does', 'to', 'him', 'll', 'hers', 'ma', 'himself', 'more', 'until', 'few', 'yours', 'but', 'about', \"haven't\", \"aren't\", 'which', 'i', \"weren't\", 'shouldn', 'what', 'just', 'on', 'do', 'itself', 'some', 'and', 'above', 'off', 'have', 'below', 'her', 'through', 'where', 'wouldn', 'the', 'once', 'because', 'before', 'than', 'm', \"don't\", \"it's\", 'she', 'hadn', \"isn't\", \"wouldn't\", 'their', 'being', 'doesn', 'a', 'how', 'now', 'he', 'them', 'y', 't', 'we', \"should've\", 'same', 'didn', 'yourselves', 'under', 'so', 'shan', 'ours', 'such', 'isn', 'been', 'further', 're', \"you're\", 'into', 'or', 'hasn', \"you'll\", 'too', 'of', 'my', \"you've\", 'herself', 'no', 'who', 'from', 'down', 'be', 'against', 'in', 'as', 'were', 's', 'had', 'there', 'you', 'out', 'will', 'your', \"didn't\", 'doing', \"hadn't\", 'during', 'an', 'whom', 'any', 'why', 'should', 'themselves', 'when', 'can', 'with', 'theirs', 'by', 'having', 'yourself', 'other', \"shouldn't\", \"that'll\", \"shan't\", 'for', \"wasn't\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords') #only need to do this once\n",
        "stops = set(stopwords.words('english'))\n",
        "print(stops)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1gVCPXklMJ9"
      },
      "source": [
        "## WordNet Lemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "scrolled": true,
        "id": "q5ebn50GlMJ9",
        "outputId": "4e1936da-e138-4415-b400-b2da6cc29325",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deposit: deposit\n",
            "deposits: deposit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet') #do it once\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(\"deposit:\", lemmatizer.lemmatize(\"deposit\"))\n",
        "print(\"deposits:\", lemmatizer.lemmatize(\"deposits\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8q8CkS0GlMJ9"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "Add stopword removal and lemmatization to your Lesk Algorithm implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqSnr9LylMJ9"
      },
      "source": [
        "## Dataset for evaluation WSD\n",
        "\n",
        "The SemCor NLTK corpus contains text that has been tagged with WordNet sense (mostly Lemmas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "IW6SXq3wlMJ9",
        "outputId": "2c116202-a919-4050-9ead-ef22ae8eac04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]   Package semcor is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('semcor') #do this once\n",
        "from nltk.corpus import semcor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Ls2xTpGulMJ9",
        "outputId": "878241fa-529c-44ae-f286-fe260ec41e3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['brown1/tagfiles/br-a01.xml', 'brown1/tagfiles/br-a02.xml', 'brown1/tagfiles/br-a11.xml', 'brown1/tagfiles/br-a12.xml', 'brown1/tagfiles/br-a13.xml', 'brown1/tagfiles/br-a14.xml', 'brown1/tagfiles/br-a15.xml', 'brown1/tagfiles/br-b13.xml', 'brown1/tagfiles/br-b20.xml', 'brown1/tagfiles/br-c01.xml', 'brown1/tagfiles/br-c02.xml', 'brown1/tagfiles/br-c04.xml', 'brown1/tagfiles/br-d01.xml', 'brown1/tagfiles/br-d02.xml', 'brown1/tagfiles/br-d03.xml', 'brown1/tagfiles/br-d04.xml', 'brown1/tagfiles/br-e01.xml', 'brown1/tagfiles/br-e02.xml', 'brown1/tagfiles/br-e04.xml', 'brown1/tagfiles/br-e21.xml', 'brown1/tagfiles/br-e24.xml', 'brown1/tagfiles/br-e29.xml', 'brown1/tagfiles/br-f03.xml', 'brown1/tagfiles/br-f10.xml', 'brown1/tagfiles/br-f19.xml', 'brown1/tagfiles/br-f43.xml', 'brown1/tagfiles/br-g01.xml', 'brown1/tagfiles/br-g11.xml', 'brown1/tagfiles/br-g15.xml', 'brown1/tagfiles/br-h01.xml', 'brown1/tagfiles/br-j01.xml', 'brown1/tagfiles/br-j02.xml', 'brown1/tagfiles/br-j03.xml', 'brown1/tagfiles/br-j04.xml', 'brown1/tagfiles/br-j05.xml', 'brown1/tagfiles/br-j06.xml', 'brown1/tagfiles/br-j07.xml', 'brown1/tagfiles/br-j08.xml', 'brown1/tagfiles/br-j09.xml', 'brown1/tagfiles/br-j10.xml', 'brown1/tagfiles/br-j11.xml', 'brown1/tagfiles/br-j12.xml', 'brown1/tagfiles/br-j13.xml', 'brown1/tagfiles/br-j14.xml', 'brown1/tagfiles/br-j15.xml', 'brown1/tagfiles/br-j16.xml', 'brown1/tagfiles/br-j17.xml', 'brown1/tagfiles/br-j18.xml', 'brown1/tagfiles/br-j19.xml', 'brown1/tagfiles/br-j20.xml', 'brown1/tagfiles/br-j22.xml', 'brown1/tagfiles/br-j23.xml', 'brown1/tagfiles/br-j37.xml', 'brown1/tagfiles/br-j52.xml', 'brown1/tagfiles/br-j53.xml', 'brown1/tagfiles/br-j54.xml', 'brown1/tagfiles/br-j55.xml', 'brown1/tagfiles/br-j56.xml', 'brown1/tagfiles/br-j57.xml', 'brown1/tagfiles/br-j58.xml', 'brown1/tagfiles/br-j59.xml', 'brown1/tagfiles/br-j60.xml', 'brown1/tagfiles/br-j70.xml', 'brown1/tagfiles/br-k01.xml', 'brown1/tagfiles/br-k02.xml', 'brown1/tagfiles/br-k03.xml', 'brown1/tagfiles/br-k04.xml', 'brown1/tagfiles/br-k05.xml', 'brown1/tagfiles/br-k06.xml', 'brown1/tagfiles/br-k07.xml', 'brown1/tagfiles/br-k08.xml', 'brown1/tagfiles/br-k09.xml', 'brown1/tagfiles/br-k10.xml', 'brown1/tagfiles/br-k11.xml', 'brown1/tagfiles/br-k12.xml', 'brown1/tagfiles/br-k13.xml', 'brown1/tagfiles/br-k14.xml', 'brown1/tagfiles/br-k15.xml', 'brown1/tagfiles/br-k16.xml', 'brown1/tagfiles/br-k17.xml', 'brown1/tagfiles/br-k18.xml', 'brown1/tagfiles/br-k19.xml', 'brown1/tagfiles/br-k20.xml', 'brown1/tagfiles/br-k21.xml', 'brown1/tagfiles/br-k22.xml', 'brown1/tagfiles/br-k23.xml', 'brown1/tagfiles/br-k24.xml', 'brown1/tagfiles/br-k25.xml', 'brown1/tagfiles/br-k26.xml', 'brown1/tagfiles/br-k27.xml', 'brown1/tagfiles/br-k28.xml', 'brown1/tagfiles/br-k29.xml', 'brown1/tagfiles/br-l11.xml', 'brown1/tagfiles/br-l12.xml', 'brown1/tagfiles/br-m01.xml', 'brown1/tagfiles/br-m02.xml', 'brown1/tagfiles/br-n05.xml', 'brown1/tagfiles/br-p01.xml', 'brown1/tagfiles/br-r05.xml', 'brown1/tagfiles/br-r06.xml', 'brown1/tagfiles/br-r07.xml', 'brown1/tagfiles/br-r08.xml', 'brown1/tagfiles/br-r09.xml', 'brown2/tagfiles/br-e22.xml', 'brown2/tagfiles/br-e23.xml', 'brown2/tagfiles/br-e25.xml', 'brown2/tagfiles/br-e26.xml', 'brown2/tagfiles/br-e27.xml', 'brown2/tagfiles/br-e28.xml', 'brown2/tagfiles/br-e30.xml', 'brown2/tagfiles/br-e31.xml', 'brown2/tagfiles/br-f08.xml', 'brown2/tagfiles/br-f13.xml', 'brown2/tagfiles/br-f14.xml', 'brown2/tagfiles/br-f15.xml', 'brown2/tagfiles/br-f16.xml', 'brown2/tagfiles/br-f17.xml', 'brown2/tagfiles/br-f18.xml', 'brown2/tagfiles/br-f20.xml', 'brown2/tagfiles/br-f21.xml', 'brown2/tagfiles/br-f22.xml', 'brown2/tagfiles/br-f23.xml', 'brown2/tagfiles/br-f24.xml', 'brown2/tagfiles/br-f25.xml', 'brown2/tagfiles/br-f33.xml', 'brown2/tagfiles/br-f44.xml', 'brown2/tagfiles/br-g12.xml', 'brown2/tagfiles/br-g14.xml', 'brown2/tagfiles/br-g16.xml', 'brown2/tagfiles/br-g17.xml', 'brown2/tagfiles/br-g18.xml', 'brown2/tagfiles/br-g19.xml', 'brown2/tagfiles/br-g20.xml', 'brown2/tagfiles/br-g21.xml', 'brown2/tagfiles/br-g22.xml', 'brown2/tagfiles/br-g23.xml', 'brown2/tagfiles/br-g28.xml', 'brown2/tagfiles/br-g31.xml', 'brown2/tagfiles/br-g39.xml', 'brown2/tagfiles/br-g43.xml', 'brown2/tagfiles/br-g44.xml', 'brown2/tagfiles/br-h09.xml', 'brown2/tagfiles/br-h11.xml', 'brown2/tagfiles/br-h12.xml', 'brown2/tagfiles/br-h13.xml', 'brown2/tagfiles/br-h14.xml', 'brown2/tagfiles/br-h15.xml', 'brown2/tagfiles/br-h16.xml', 'brown2/tagfiles/br-h17.xml', 'brown2/tagfiles/br-h18.xml', 'brown2/tagfiles/br-h21.xml', 'brown2/tagfiles/br-h24.xml', 'brown2/tagfiles/br-j29.xml', 'brown2/tagfiles/br-j30.xml', 'brown2/tagfiles/br-j31.xml', 'brown2/tagfiles/br-j32.xml', 'brown2/tagfiles/br-j33.xml', 'brown2/tagfiles/br-j34.xml', 'brown2/tagfiles/br-j35.xml', 'brown2/tagfiles/br-j38.xml', 'brown2/tagfiles/br-j41.xml', 'brown2/tagfiles/br-j42.xml', 'brown2/tagfiles/br-l08.xml', 'brown2/tagfiles/br-l09.xml', 'brown2/tagfiles/br-l10.xml', 'brown2/tagfiles/br-l13.xml', 'brown2/tagfiles/br-l14.xml', 'brown2/tagfiles/br-l15.xml', 'brown2/tagfiles/br-l16.xml', 'brown2/tagfiles/br-l17.xml', 'brown2/tagfiles/br-l18.xml', 'brown2/tagfiles/br-n09.xml', 'brown2/tagfiles/br-n10.xml', 'brown2/tagfiles/br-n11.xml', 'brown2/tagfiles/br-n12.xml', 'brown2/tagfiles/br-n14.xml', 'brown2/tagfiles/br-n15.xml', 'brown2/tagfiles/br-n16.xml', 'brown2/tagfiles/br-n17.xml', 'brown2/tagfiles/br-n20.xml', 'brown2/tagfiles/br-p07.xml', 'brown2/tagfiles/br-p09.xml', 'brown2/tagfiles/br-p10.xml', 'brown2/tagfiles/br-p12.xml', 'brown2/tagfiles/br-p24.xml', 'brown2/tagfiles/br-r04.xml', 'brownv/tagfiles/br-a03.xml', 'brownv/tagfiles/br-a04.xml', 'brownv/tagfiles/br-a05.xml', 'brownv/tagfiles/br-a06.xml', 'brownv/tagfiles/br-a07.xml', 'brownv/tagfiles/br-a08.xml', 'brownv/tagfiles/br-a09.xml', 'brownv/tagfiles/br-a10.xml', 'brownv/tagfiles/br-a16.xml', 'brownv/tagfiles/br-a17.xml', 'brownv/tagfiles/br-a18.xml', 'brownv/tagfiles/br-a19.xml', 'brownv/tagfiles/br-a20.xml', 'brownv/tagfiles/br-a21.xml', 'brownv/tagfiles/br-a22.xml', 'brownv/tagfiles/br-a23.xml', 'brownv/tagfiles/br-a24.xml', 'brownv/tagfiles/br-a25.xml', 'brownv/tagfiles/br-a26.xml', 'brownv/tagfiles/br-a27.xml', 'brownv/tagfiles/br-a28.xml', 'brownv/tagfiles/br-a29.xml', 'brownv/tagfiles/br-a30.xml', 'brownv/tagfiles/br-a31.xml', 'brownv/tagfiles/br-a32.xml', 'brownv/tagfiles/br-a33.xml', 'brownv/tagfiles/br-a34.xml', 'brownv/tagfiles/br-a35.xml', 'brownv/tagfiles/br-a36.xml', 'brownv/tagfiles/br-a37.xml', 'brownv/tagfiles/br-a38.xml', 'brownv/tagfiles/br-a39.xml', 'brownv/tagfiles/br-a40.xml', 'brownv/tagfiles/br-a41.xml', 'brownv/tagfiles/br-a42.xml', 'brownv/tagfiles/br-a43.xml', 'brownv/tagfiles/br-a44.xml', 'brownv/tagfiles/br-b01.xml', 'brownv/tagfiles/br-b02.xml', 'brownv/tagfiles/br-b03.xml', 'brownv/tagfiles/br-b04.xml', 'brownv/tagfiles/br-b05.xml', 'brownv/tagfiles/br-b06.xml', 'brownv/tagfiles/br-b07.xml', 'brownv/tagfiles/br-b08.xml', 'brownv/tagfiles/br-b09.xml', 'brownv/tagfiles/br-b10.xml', 'brownv/tagfiles/br-b11.xml', 'brownv/tagfiles/br-b12.xml', 'brownv/tagfiles/br-b14.xml', 'brownv/tagfiles/br-b15.xml', 'brownv/tagfiles/br-b16.xml', 'brownv/tagfiles/br-b17.xml', 'brownv/tagfiles/br-b18.xml', 'brownv/tagfiles/br-b19.xml', 'brownv/tagfiles/br-b21.xml', 'brownv/tagfiles/br-b22.xml', 'brownv/tagfiles/br-b23.xml', 'brownv/tagfiles/br-b24.xml', 'brownv/tagfiles/br-b25.xml', 'brownv/tagfiles/br-b26.xml', 'brownv/tagfiles/br-b27.xml', 'brownv/tagfiles/br-c03.xml', 'brownv/tagfiles/br-c05.xml', 'brownv/tagfiles/br-c06.xml', 'brownv/tagfiles/br-c07.xml', 'brownv/tagfiles/br-c08.xml', 'brownv/tagfiles/br-c09.xml', 'brownv/tagfiles/br-c10.xml', 'brownv/tagfiles/br-c11.xml', 'brownv/tagfiles/br-c12.xml', 'brownv/tagfiles/br-c13.xml', 'brownv/tagfiles/br-c14.xml', 'brownv/tagfiles/br-c15.xml', 'brownv/tagfiles/br-c16.xml', 'brownv/tagfiles/br-c17.xml', 'brownv/tagfiles/br-d05.xml', 'brownv/tagfiles/br-d06.xml', 'brownv/tagfiles/br-d07.xml', 'brownv/tagfiles/br-d08.xml', 'brownv/tagfiles/br-d09.xml', 'brownv/tagfiles/br-d10.xml', 'brownv/tagfiles/br-d11.xml', 'brownv/tagfiles/br-d12.xml', 'brownv/tagfiles/br-d13.xml', 'brownv/tagfiles/br-d14.xml', 'brownv/tagfiles/br-d15.xml', 'brownv/tagfiles/br-d16.xml', 'brownv/tagfiles/br-d17.xml', 'brownv/tagfiles/br-e03.xml', 'brownv/tagfiles/br-e05.xml', 'brownv/tagfiles/br-e06.xml', 'brownv/tagfiles/br-e07.xml', 'brownv/tagfiles/br-e08.xml', 'brownv/tagfiles/br-e09.xml', 'brownv/tagfiles/br-e10.xml', 'brownv/tagfiles/br-e11.xml', 'brownv/tagfiles/br-e12.xml', 'brownv/tagfiles/br-e13.xml', 'brownv/tagfiles/br-e14.xml', 'brownv/tagfiles/br-e15.xml', 'brownv/tagfiles/br-e16.xml', 'brownv/tagfiles/br-e17.xml', 'brownv/tagfiles/br-e18.xml', 'brownv/tagfiles/br-e19.xml', 'brownv/tagfiles/br-e20.xml', 'brownv/tagfiles/br-f01.xml', 'brownv/tagfiles/br-f02.xml', 'brownv/tagfiles/br-f04.xml', 'brownv/tagfiles/br-f05.xml', 'brownv/tagfiles/br-f06.xml', 'brownv/tagfiles/br-f07.xml', 'brownv/tagfiles/br-f09.xml', 'brownv/tagfiles/br-f11.xml', 'brownv/tagfiles/br-f12.xml', 'brownv/tagfiles/br-g02.xml', 'brownv/tagfiles/br-g03.xml', 'brownv/tagfiles/br-g04.xml', 'brownv/tagfiles/br-g05.xml', 'brownv/tagfiles/br-g06.xml', 'brownv/tagfiles/br-g07.xml', 'brownv/tagfiles/br-g08.xml', 'brownv/tagfiles/br-g09.xml', 'brownv/tagfiles/br-g10.xml', 'brownv/tagfiles/br-g13.xml', 'brownv/tagfiles/br-h02.xml', 'brownv/tagfiles/br-h03.xml', 'brownv/tagfiles/br-h04.xml', 'brownv/tagfiles/br-h05.xml', 'brownv/tagfiles/br-h06.xml', 'brownv/tagfiles/br-h07.xml', 'brownv/tagfiles/br-h08.xml', 'brownv/tagfiles/br-h10.xml', 'brownv/tagfiles/br-j21.xml', 'brownv/tagfiles/br-j24.xml', 'brownv/tagfiles/br-j25.xml', 'brownv/tagfiles/br-j26.xml', 'brownv/tagfiles/br-j27.xml', 'brownv/tagfiles/br-j28.xml', 'brownv/tagfiles/br-l01.xml', 'brownv/tagfiles/br-l02.xml', 'brownv/tagfiles/br-l03.xml', 'brownv/tagfiles/br-l04.xml', 'brownv/tagfiles/br-l05.xml', 'brownv/tagfiles/br-l06.xml', 'brownv/tagfiles/br-l07.xml', 'brownv/tagfiles/br-m03.xml', 'brownv/tagfiles/br-m04.xml', 'brownv/tagfiles/br-m05.xml', 'brownv/tagfiles/br-m06.xml', 'brownv/tagfiles/br-n01.xml', 'brownv/tagfiles/br-n02.xml', 'brownv/tagfiles/br-n03.xml', 'brownv/tagfiles/br-n04.xml', 'brownv/tagfiles/br-n06.xml', 'brownv/tagfiles/br-n07.xml', 'brownv/tagfiles/br-n08.xml', 'brownv/tagfiles/br-p02.xml', 'brownv/tagfiles/br-p03.xml', 'brownv/tagfiles/br-p04.xml', 'brownv/tagfiles/br-p05.xml', 'brownv/tagfiles/br-p06.xml', 'brownv/tagfiles/br-p08.xml', 'brownv/tagfiles/br-r01.xml', 'brownv/tagfiles/br-r02.xml', 'brownv/tagfiles/br-r03.xml']\n"
          ]
        }
      ],
      "source": [
        "# Get a list of file identifiers in SemCor\n",
        "file_ids = semcor.fileids()\n",
        "print(file_ids) #looks like they're from the brown dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "fvwhmamWlMJ-",
        "outputId": "175576d6-b003-4283-9d9e-2d40a99468c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', 'Atlanta', \"'s\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term', 'end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]\n"
          ]
        }
      ],
      "source": [
        "# Access the sense-tagged sentences from a file\n",
        "sentences = semcor.sents(file_ids[0])\n",
        "print(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "-gEfY7helMJ-",
        "outputId": "d24f539f-cd38-49e8-86b6-2303439053fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[['The'], Tree(Lemma('group.n.01.group'), [Tree('NE', ['Fulton', 'County', 'Grand', 'Jury'])]), Tree(Lemma('state.v.01.say'), ['said']), Tree(Lemma('friday.n.01.Friday'), ['Friday']), ['an'], Tree(Lemma('probe.n.01.investigation'), ['investigation']), ['of'], Tree(Lemma('atlanta.n.01.Atlanta'), ['Atlanta']), [\"'s\"], Tree(Lemma('late.s.03.recent'), ['recent']), Tree(Lemma('primary.n.01.primary_election'), ['primary', 'election']), Tree(Lemma('produce.v.04.produce'), ['produced']), ['``'], ['no'], Tree(Lemma('evidence.n.01.evidence'), ['evidence']), [\"''\"], ['that'], ['any'], Tree(Lemma('abnormality.n.04.irregularity'), ['irregularities']), Tree(Lemma('happen.v.01.take_place'), ['took', 'place']), ['.']], [['The'], Tree(Lemma('jury.n.01.jury'), ['jury']), Tree(Lemma('far.r.02.far'), ['further']), Tree(Lemma('state.v.01.say'), ['said']), ['in'], Tree(Lemma('term.n.02.term'), ['term']), Tree(Lemma('end.n.02.end'), ['end']), Tree(Lemma('presentment.n.01.presentment'), ['presentments']), ['that'], ['the'], Tree(Lemma('group.n.01.group'), [Tree('NE', ['City', 'Executive', 'Committee'])]), [','], ['which'], Tree(Lemma('own.v.01.have'), ['had']), Tree(Lemma('overall.s.02.overall'), ['over-all']), Tree(Lemma('mission.n.03.charge'), ['charge']), ['of'], ['the'], Tree(Lemma('election.n.01.election'), ['election']), [','], ['``'], Tree(Lemma('deserve.v.01.deserve'), ['deserves']), ['the'], Tree(Lemma('praise.n.01.praise'), ['praise']), ['and'], Tree(Lemma('thanks.n.01.thanks'), ['thanks']), ['of'], ['the'], Tree(Lemma('location.n.01.location'), [Tree('NE', ['City', 'of', 'Atlanta'])]), [\"''\"], ['for'], ['the'], Tree(Lemma('manner.n.01.manner'), ['manner']), ['in'], ['which'], ['the'], Tree(Lemma('election.n.01.election'), ['election']), ['was'], Tree(Lemma('conduct.v.01.conduct'), ['conducted']), ['.']], ...]\n"
          ]
        }
      ],
      "source": [
        "# Access the sense tags for those sentences\n",
        "tags = semcor.tagged_sents(file_ids[0],tag=\"sem\")\n",
        "print(tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIW_4_volMJ-"
      },
      "source": [
        "This is a complex format - notice that some (but not all!) of the words are grouped together in a tree structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "MsI06GcwlMJ-",
        "outputId": "cd6044ec-31bf-431d-c238-bda0d1a22e90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The']\n",
            "(Lemma('group.n.01.group') (NE Fulton County Grand Jury))\n",
            "(Lemma('state.v.01.say') said)\n",
            "(Lemma('friday.n.01.Friday') Friday)\n",
            "['an']\n",
            "(Lemma('probe.n.01.investigation') investigation)\n",
            "['of']\n",
            "(Lemma('atlanta.n.01.Atlanta') Atlanta)\n",
            "[\"'s\"]\n",
            "(Lemma('late.s.03.recent') recent)\n",
            "(Lemma('primary.n.01.primary_election') primary election)\n",
            "(Lemma('produce.v.04.produce') produced)\n",
            "['``']\n",
            "['no']\n",
            "(Lemma('evidence.n.01.evidence') evidence)\n",
            "[\"''\"]\n",
            "['that']\n",
            "['any']\n",
            "(Lemma('abnormality.n.04.irregularity') irregularities)\n",
            "(Lemma('happen.v.01.take_place') took place)\n",
            "['.']\n"
          ]
        }
      ],
      "source": [
        "# tags[0] is the tags for the first sentence, sentence[0]\n",
        "for tag in tags[0]:\n",
        "    print(tag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMBPqPhelMJ-"
      },
      "source": [
        "Notice\n",
        "* Some tokens don't have a tag - stopwords, punctuation, etc. - these show up as a string inside a list\n",
        "* \"Fulton County Grand Jury\" is grouped under Lemma('group.n.01.group')\n",
        "* \"primary election\" is grouped as a compound word with Lemma('primary.n.01.primary_election')\n",
        "\n",
        "This is going to be tough to work with. Here's an attempt to loop through them, match them up wit the word from the sentence, and handle these issues."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_ids = semcor.fileids()\n",
        "sentences = semcor.sents(file_ids[0])\n",
        "tags = semcor.tagged_sents(file_ids[0], tag=\"sem\")"
      ],
      "metadata": {
        "id": "wCPEBRX9kJKZ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "0ue2d2qwlMJ-",
        "outputId": "52610ae7-6e39-4c43-ba0a-6d5d22d5d65a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: The\n",
            "Tag: ['The']\n",
            "Word: Fulton\n",
            "Tag: (Lemma('group.n.01.group') (NE Fulton County Grand Jury))\n",
            "Words in this group: ['Fulton', 'County', 'Grand', 'Jury']\n",
            "Word: said\n",
            "Tag: (Lemma('state.v.01.say') said)\n",
            "{'recent', 'place', \"'s\", 'jury', 'friday', 'fulton', '``', 'atlanta', 'took', 'said', 'grand', 'primary', 'irregularity', 'county', 'evidence', 'produced', 'election', 'investigation'}\n",
            "[Synset('state.v.01'), Synset('allege.v.01'), Synset('suppose.v.01'), Synset('read.v.02'), Synset('order.v.01'), Synset('pronounce.v.01'), Synset('say.v.07'), Synset('say.v.08'), Synset('say.v.09'), Synset('say.v.10'), Synset('say.v.11'), Synset('aforesaid.s.01')]\n",
            "{'bothering', 'tell', 'wanted', 'express', 'opinion', 'marry', 'state', 'said', 'word', 'name'}\n",
            "1\n",
            "{'victim', 'late', 'school', 'alleged', 'report', 'say', 'crime', 'intervene', 'said', 'owe', 'war', 'registrar', 'money', 'maintain'}\n",
            "1\n",
            "{'tell', \"'s\", 'would', 'truth', 'let', 'u', 'say', 'lot', '--', 'money', 'express', 'supposition', '?'}\n",
            "1\n",
            "{'contain', 'passage', 'read', 'certain', 'follows', 'say', 'law', 'wording', '?', 'form'}\n",
            "0\n",
            "{'direct', 'somebody', 'instruction', 'give', 'authority', 'dressed', 'said', 'go', 'something', 'get', 'shopping', 'told', 'home', 'child', 'ordered', 'mother'}\n",
            "1\n",
            "{\"'\", 'say', 'pronounces', 'sound', 'french', 'zip', 'funny', 'word', 'child', 'speak', 'way', 'pronounce', '`', 'complicated', 'certain', 'utter', ',', 'wire', '?'}\n",
            "0\n",
            "{'anything', 'communicate', 'felt', '?', 'nonverbally', 'say', 'face', 'express', 'painting'}\n",
            "0\n",
            "{'office', \"'\", 'utter', 'said', '`', 'hello', 'aloud', 'everyone'}\n",
            "1\n",
            "{'declare', \"'s\", 'one', 'forget', 'business', 'let', 'opinion', 'state', 'say', 'judgement', 'whole', ';'}\n",
            "1\n",
            "{'grace', 'hail', \"'\", 'fixed', 'text', 'said', 'say', 'repeat', '`', 'recite', 'mary'}\n",
            "1\n",
            "{'clock', 'say', 'noon', 'indicate'}\n",
            "0\n",
            "{'spoken', 'party', 'one', 'charge', 'work', 'author', 'said', 'previously', 'denied', 'mentioned', 'aforementioned'}\n",
            "1\n",
            "Word: Friday\n",
            "Tag: (Lemma('friday.n.01.Friday') Friday)\n",
            "{'recent', 'place', \"'s\", 'jury', 'friday', 'fulton', '``', 'atlanta', 'took', 'said', 'grand', 'primary', 'irregularity', 'county', 'evidence', 'produced', 'election', 'investigation'}\n",
            "[Synset('friday.n.01')]\n",
            "{'week', 'fifth', 'day', 'sixth', 'working', ';'}\n",
            "0\n",
            "Word: an\n",
            "Tag: ['an']\n",
            "Word: investigation\n",
            "Tag: (Lemma('probe.n.01.investigation') investigation)\n",
            "{'recent', 'place', \"'s\", 'jury', 'friday', 'fulton', '``', 'atlanta', 'took', 'said', 'grand', 'primary', 'irregularity', 'county', 'evidence', 'produced', 'election', 'investigation'}\n",
            "[Synset('probe.n.01'), Synset('investigation.n.02')]\n",
            "{'congressional', 'questionable', 'unfamiliar', 'scandal', 'probe', 'inquiry', 'activity'}\n",
            "0\n",
            "{'work', 'inquiring', 'systematically', 'something', 'thoroughly'}\n",
            "0\n",
            "Word: of\n",
            "Tag: ['of']\n",
            "Word: Atlanta\n",
            "Tag: (Lemma('atlanta.n.01.Atlanta') Atlanta)\n",
            "{'recent', 'place', \"'s\", 'jury', 'friday', 'fulton', '``', 'atlanta', 'took', 'said', 'grand', 'primary', 'irregularity', 'county', 'evidence', 'produced', 'election', 'investigation'}\n",
            "[Synset('atlanta.n.01'), Synset('atlanta.n.02')]\n",
            "{'army', 'Civil', 'plundered', ';', 'commercial', 'chief', 'city', 'state', 'United', 'States', 'War', 'southeastern', 'American', 'Georgia', 'Sherman', 'capital', \"'s\", 'largest', 'burned', 'center'}\n",
            "1\n",
            "{'railroad', 'supplying', 'Federal', 'city', 'cut', 'siege', 'burned', 'Sherman', 'troop', ';', '1864'}\n",
            "0\n",
            "Word: 's\n",
            "Tag: [\"'s\"]\n",
            "Word: recent\n",
            "Tag: (Lemma('late.s.03.recent') recent)\n",
            "{'recent', 'place', \"'s\", 'jury', 'friday', 'fulton', '``', 'atlanta', 'took', 'said', 'grand', 'primary', 'irregularity', 'county', 'evidence', 'produced', 'election', 'investigation'}\n",
            "[Synset('holocene.n.01'), Synset('recent.s.01'), Synset('late.s.03')]\n",
            "{'last', 'approximately', '10,000', 'year'}\n",
            "0\n",
            "{'addition', 'graduate', 'new', 'bud', 'tree', 'house', 'recent', 'apple'}\n",
            "1\n",
            "{'late', 'trip', 'time', 'journal', 'quarrel', 'development', 'previous', 'africa', 'past', 'immediate', 'present', 'issue', 'recent', 'month'}\n",
            "1\n",
            "Word: ['primary']\n",
            "Tag: (Lemma('primary.n.01.primary_election') primary election)\n",
            "Word: produced\n",
            "Tag: (Lemma('produce.v.04.produce') produced)\n",
            "{'recent', 'place', \"'s\", 'jury', 'friday', 'fulton', '``', 'atlanta', 'took', 'said', 'grand', 'primary', 'irregularity', 'county', 'evidence', 'produced', 'election', 'investigation'}\n",
            "[Synset('produce.v.01'), Synset('produce.v.02'), Synset('produce.v.03'), Synset('produce.v.04'), Synset('grow.v.07'), Synset('produce.v.06'), Synset('grow.v.08')]\n",
            "{'forth', 'would', 'fruit', 'yield', 'produce', 'tree', 'bring'}\n",
            "0\n",
            "{'car', 'two', 'produce', 'create', 'product', 'toy', 'sell', 'making', 'manufacture', 'company', 'man-made', 'century'}\n",
            "0\n",
            "{'procedure', 'change', 'cause', 'gave', 'many', 'law', 'exist', 'system', 'bring', 'vapor', 'produce', 'complaint', 'noxious', 'president', 'rise', 'happen', 'occur', 'effect', 'health', 'must', 'curious', ',', 'new', 'care', 'chemical'}\n",
            "0\n",
            "{'court', 'forth', 'brought', 'father', 'produced', 'many', 'exonerates', 'bring', 'claim', 'picture', 'display', 'proud', 'accused', 'baby', 'letter'}\n",
            "1\n",
            "{'wine', 'raise', 'improvement', 'bordeaux', 'grow', 'hog', 'parma', 'cultivate', 'mean', 'great', 'red', 'technique', 'growing', 'region', 'produce', 'ham', 'involving', 'good', 'often', ',', 'agricultural', 'wheat'}\n",
            "0\n",
            "{'onto', 'produce', 'play', 'release', 'new', 'book', 'bring', 'market', 'movie'}\n",
            "0\n",
            "{'feature', 'change', ')', 'spot', 'come', 'patient', 'attribute', 'well-developed', 'abdominal', 'got', 'undergo', 'funny', '(', 'body', 'beard', 'developed', 'physical', 'grew', 'breast', 'pain'}\n",
            "0\n",
            "Word: ``\n",
            "Tag: ['``']\n",
            "Word: no\n",
            "Tag: ['no']\n",
            "Word: evidence\n",
            "Tag: (Lemma('evidence.n.01.evidence') evidence)\n",
            "{'recent', 'place', \"'s\", 'jury', 'friday', 'fulton', '``', 'atlanta', 'took', 'said', 'grand', 'primary', 'irregularity', 'county', 'evidence', 'produced', 'election', 'investigation'}\n",
            "[Synset('evidence.n.01'), Synset('evidence.n.02'), Synset('evidence.n.03'), Synset('attest.v.01'), Synset('testify.v.02'), Synset('tell.v.07')]\n",
            "{'compelling', 'cause', 'knowledge', 'belief', 'cancer', 'base', 'basis', 'smoking', 'evidence', 'disbelief', 'lung', ';'}\n",
            "1\n",
            "{'indication', 'make', 'something', 'evident', 'trembling', 'evidence', 'fear'}\n",
            "1\n",
            "{'established', 'judicial', ')', 'truth', 'mean', '(', 'trial', 'alleged', 'matter', 'law', 'whose', 'disproved', 'fact', 'investigated'}\n",
            "0\n",
            "{'high', 'sense', 'attribute', 'rome', ';', 'stand', 'decision', 'show', 'architectural', 'fever', 'manifest', 'building', 'level', 'behavior', 'one', 'illness', 'attested', 'evidence', 'demonstrates', 'fairness', 'provide', \"'s\", 'proof', ',', 'external', 'sophistication', 'attitude'}\n",
            "2\n",
            "{'behavior', 'blood', 'test', 'showed', 'testified', 'evidence', 'incompetence', 'father', 'provide'}\n",
            "1\n",
            "{'give', 'former', 'evidence', 'colleague', 'telling'}\n",
            "1\n",
            "Word: ''\n",
            "Tag: [\"''\"]\n",
            "Word: that\n",
            "Tag: ['that']\n",
            "Word: any\n",
            "Tag: ['any']\n",
            "Word: irregularities\n",
            "Tag: (Lemma('abnormality.n.04.irregularity') irregularities)\n",
            "{'recent', 'place', \"'s\", 'jury', 'friday', 'fulton', '``', 'atlanta', 'took', 'said', 'grand', 'primary', 'irregularity', 'county', 'evidence', 'produced', 'election', 'investigation'}\n",
            "[Synset('abnormality.n.04'), Synset('irregularity.n.02'), Synset('irregularity.n.03'), Synset('constipation.n.01')]\n",
            "{'behavior', 'morality', 'rule', 'breach', 'custom', 'etiquette'}\n",
            "0\n",
            "{';', 'fixed', 'irregular', 'interval', 'principle', 'rate', 'characterized'}\n",
            "0\n",
            "{'pattern', 'irregular', 'spatial', 'asymmetry', ';', 'shape'}\n",
            "0\n",
            "{'difficult', 'obstruction', 'diverticulitis', 'intestinal', 'evacuation', 'irregular', 'infrequent', 'symptom', 'bowel', ';'}\n",
            "0\n",
            "Word: ['took']\n",
            "Tag: (Lemma('happen.v.01.take_place') took place)\n",
            "Word: .\n",
            "Tag: ['.']\n"
          ]
        }
      ],
      "source": [
        "# for keeping track of which word and tag we're on\n",
        "word_idx = 0\n",
        "tag_idx = 0\n",
        "\n",
        "while tag_idx < len(tags[0]) and word_idx < len(sentences[0]):\n",
        "    word = sentences[0][word_idx] #the current word\n",
        "    tag = tags[0][tag_idx] #the tag for the current word\n",
        "\n",
        "    # check for tags that got assigned to compound words like primary_election\n",
        "    if len(tag) > 1:\n",
        "        print(\"Word:\",sentences[0][word_idx:(word_idx+len(tag)-1)])\n",
        "        print(\"Tag:\",tag)\n",
        "        word_idx += len(tag) #move to the next word that isn't part of the compound\n",
        "\n",
        "    # for Tree objects, check if it really tagged a word and not a group\n",
        "    elif type(tag) is nltk.Tree and type(tag[0]) is str:\n",
        "        print(\"Word:\",word)\n",
        "        print(\"Tag:\",tag)\n",
        "\n",
        "        # here's how we can get the synset for tags that give us a Lemma\n",
        "        if  type(tag.label()) != str:\n",
        "            actual_sense = tag.label().synset()\n",
        "            pred_sense = simplified_lesk(word,sentences[0])\n",
        "            #this is where you could check if you correctly matched the actual sense\n",
        "\n",
        "        word_idx += 1 #advance to next word\n",
        "\n",
        "    # check if it's a punctuation/stopword - if we got here, it means tag was not of type nltk.Tree\n",
        "    elif type(tag[0]) is str:\n",
        "        print(\"Word:\",word)\n",
        "        print(\"Tag:\",tag)\n",
        "        word_idx += 1\n",
        "\n",
        "    # If we get gerem it means the Tree contained a group of words, and we can count\n",
        "    # how many with len( tag.leaves() )\n",
        "    else:\n",
        "        print(\"Word:\",word)\n",
        "        print(\"Tag:\",tag)\n",
        "        print(\"Words in this group:\",tag.leaves())\n",
        "        word_idx += len(tag.leaves())\n",
        "    tag_idx += 1\n",
        "    # print()\n",
        "    # print(pred_sense)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-uK-2JxlMJ-"
      },
      "source": [
        "## Applied Exploration\n",
        "\n",
        "For cases where the SemCor dataset has a single word tagged with a WordNet sense, run your `simplified_lesk` code on it and see if it matches. Go through all of the sentences in a particular file_id and compute an accuracy score.\n",
        "\n",
        "Write notes here on what you did and the results you got.\n",
        "\n",
        "the loop is going through a lot more and is breaking each word and seeing if it has a match with the word in the tag by depending the word it has different meaning"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}